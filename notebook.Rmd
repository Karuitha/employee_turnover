---
title: "EMPLOYEE TURNOVER"
author: "John Karuitha & Remon Hanna"
date: "`r format(Sys.Date(), format = '%A %B %d, %Y')`"
subtitle: "Can Firms use `Machine Learning` to Manage Employee Turnover?"
output: 
  html_document:
    theme: darkly
    highlight: tango 
    code_folding: hide
    toc: true
    toc_float: true
    number_sections: true
bibliography: citations.bib
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)

if(!require(pacman)){
  
  install.packages("pacman")
  
}

pacman::p_load(tidyverse, kableExtra, skimr, knitr, glue, GGally, 
               
               corrplot, tidymodels, themis, stargazer, rpart, rpart.plot, 
               
               vip, patchwork, data.table, kknn, gghalves)

```

<style>
table {
  background-color: white !important;
  color: black !important;
}
</style>


# ðŸ“– Background

The Board of ABC Inc. is worried about the relatively high employee turnover. To arrest this situation, the board has tasked the Human Resources department to look into ways to reduce the number of employees leaving the company.

The HR manager formed a team to examine this matter, and the authors of this article are part of this team. The team needs to understand better the situation, which employees are more likely to leave, and why. Once it is clear what variables impact employee churn, the team shall present the findings and ideas to the board on how to attack the problem.

# ðŸ’ª Objectives

As part of the team, we set forth to answer the following questions:

1. Which department has the highest employee turnover? Which one has the lowest?
2. Which variables seem to be better predictors of employee departure?
3. What recommendations would you make regarding ways to reduce employee turnover?
4. Which model(s) have the highest predictive power for employees likely to leave the company?

# Key Insights

1. `IT` department has the highest staff turnover, followed closely by the `logistics`.
2. `Finance` department has the lowest incidence of staff turnover with `admin` a close second. 
3. There is high positive correlation between `tenure` which captures the length of time an employee has been with the organisation and the `avg_hours_month`, the average number of hours an employee clocks per month. 

# Recommendations

# Project Scope

There are numerous machine learning models that we could use to tackle this problem. To manage the scope of the project, we concentrate on the following models. 

1. Logistic regression model. 
2. Decision tree model.
3. Random forest model. 
4. K-Nearest neighbours model (KNN).
5. XG-Boost model.
5. Support vector machine (SVM) model.
6. An ensemble of all the above models. 

# ðŸ’¾ The data
The department has assembled data on almost 10,000 employees. The team gathered the data using information from exit interviews, performance reviews, and other employee records. The data consists of the following variables. 

```{r}
tribble(~ Variable, ~ Description,
        
        "department", "The department the employee belongs.",
        
        "promoted", "1 if the employee was promoted in the previous 24 months, 0 otherwise.", 
        
        "review", "The composite score the employee received in their last evaluation.", 
        
        "projects", "How many projects the employee is involved.", 
        
        "salary", "For confidentiality reasons, salary comes in three tiers: low, medium, and high.", 
        
        "tenure", "How many years the employee has been at the company.", 
        
        "satisfaction", "a measure of employee satisfaction from surveys.", 
        
        "avg_hrs_month", "the average hours the employee worked in a month.", 
        
        "left", "'yes' if the employee ended up leaving, 'no' otherwise.") %>% 
  
  kbl(., booktabs = TRUE, 
      
      caption = "Variables Description") %>% 
  
  kable_classic(full_width = TRUE, font_size = 12, 
                
                bootstrap_options = "striped")
```



## Exploring the Data

We read the data from a csv file provided. You can download the data from this [link](https://raw.githubusercontent.com/Karuitha/data_projects/master/employee_turnover/data/employee_churn_data.csv) or copy and paste the following web address <https://raw.githubusercontent.com/Karuitha/data_projects/master/employee_turnover/data/employee_churn_data.csv>.


```{r message = FALSE}

my_col_names <- c("department", "promoted", "review", "projects", "salary", "tenure", "satisfaction", "bonus", "avg_hrs_month", "left")

df <- fread('./data/employee_churn_data.csv') %>%

  mutate(left = factor(left, levels = c("yes", "no")))

head(df) %>% 
  
  kbl(., booktabs = TRUE, 
      
      caption = "First Six Rows of the HR Data") %>% 
  
  kable_classic(full_width = TRUE, font_size = 12, 
                
                bootstrap_options = "striped")

```


We then examine the data, starting with missing values and possible duplicates. 

```{r}
sapply(df, is.na) %>%
  
  colSums() %>%
  
  tibble(variables = names(df), missing = .) %>%
  
  arrange(desc(missing)) %>% 
  
  kbl(., booktabs = TRUE, 
      
      caption = "Missing Data") %>% 
  
  kable_classic(full_width = TRUE, font_size = 12, 
                
                bootstrap_options = "striped")

```

```{r}
df %>%
  
  filter(duplicated(.))
```

The data set has no missing values and duplicated records.We next delve into the analysis. 

Figure () below shows a pairs plot between the ten variables in the dataset. Two issues come up particularly strongly from the plot.

1. There is a low prevalence of workers who left versus those that remain in the organisation. Prevalence can have impact on the generalizability of machine learning models. Hence, it is always good to ensure data balance by up sampling or down sampling the data. 

2. There is an extremely strong correlation between `tenure` and `working hours per month`. The observation means that workers that have stayed longer in the organisation tend to put in more hours. The implication is that workers who remain in the organisation have a higher motivation to work. 

```{r, fig.width = 12, fig.height = 8, fig.cap = "Visualisation of the Variables", warning = FALSE, message = FALSE}


df %>%
  
  GGally::ggpairs(ggplot2::aes(col = left, fill = left)) + 
  
  scale_color_manual(values = c("gray", "orange")) + 
  
  scale_fill_viridis_d(option = "magma")
  
```


The charts below show the relationship between several variables with employee churn. 

```{r, fig.width = 12, fig.height = 8, fig.cap = "Visualisation of the Variables", warning = FALSE, message = FALSE}
(df %>% 
  
  ggplot(mapping = aes(x = left, y = review, fill = left)) + 
  
  geom_boxplot(show.legend = FALSE) + 
  
  scale_fill_viridis_d(option = "magma") + 
  
  labs(x = "Left", y = "Review", 
       
       title = "Review Scores and Employee Churn") +


df %>% 
  
  ggplot(mapping = aes(x = left, y = avg_hrs_month, 
                       
                       fill = left)) + 
  
  geom_boxplot(show.legend = FALSE) +
  
  scale_fill_viridis_d(option = "magma") + 
  
  labs(x = "Left", y = "Average Hours Worked per Month", 
       
       title = "Hours Worked and Employee Churn")) /



(df %>% 
  
  ggplot(mapping = aes(x = promoted, y = ..count..)) + 
  
  geom_bar(aes(fill = factor(left)), 
           
           position = "fill", show.legend = FALSE) + 
  
  scale_fill_viridis_d(option = "magma") + 
  
  labs(x = "Promoted", y = "Count", 
       
       title = "Promotion and Employee Churn") +


df %>% 
  
  ggplot(mapping = aes(x = department, y = ..count..)) + 
  
  geom_bar(aes(fill = factor(left)), 
           
           position = "fill", show.legend = FALSE) + 
  
  scale_fill_viridis_d(option = "magma") + 
  
  labs(x = "Department", y = "Count", 
       
       title = "Department and Employee Churn") + 
  
  coord_flip())
```


```{r}
df %>% 
  
  ggplot(mapping = aes(x = left, 
                       
                       y = satisfaction,
                       
                       fill = left)) + 
  
  geom_violin(width=0.6) +
  
    geom_boxplot(width=0.1, color="grey", alpha=0.2) +
  
  scale_fill_viridis_d(option = "magma", alpha = 0.5) + 
  
  labs(x = "Left?", y = "Satisfaction", 
       
       title = "Satisfaction and Employee Churn")
  
  
```

## Summary Statistics

In this section, We summarise the data and present the correlation matrix.

The correlation matrix in Table () shows a very high correlation between `average hours per month` worked and `tenure`as noted earlier in figure (). The data further shows some substantial correlation between `tenure` and `review`, `satisfaction` and `review`, `average hours per month` and review`, `satisfaction` and `tenure`, and finally, `average hours per month` and `satisfaction`. 

```{r}

df %>%
  
  select(where(is.numeric)) %>%
  
  cor() %>%
  
  corrplot(type = "lower")
```

Table () below shows the summary statistics for the numeric predictor variables. 

```{r}

df %>%
  
  select(where(is.numeric)) %>%
  
  skimr::skim_without_charts() %>%
  
  select(-n_missing, -complete_rate, -skim_type) %>%
  
  rename(Variable = skim_variable, Mean = numeric.mean,
         
         SD = numeric.sd, Min = numeric.p0, Q1 = numeric.p25,
         
         Median = numeric.p50, Q3 = numeric.p75, 
         
         Max = numeric.p100) %>%
  
  kbl(., booktabs = TRUE, caption = "Summary Statistics") %>%
  
  kable_classic(position = "left")
```


```{r}

df %>%
  
  select(where(is.character)) %>%
  
  skimr::skim_without_charts() %>%
  
  select(-n_missing, -complete_rate, -skim_type) %>%
  
  rename(Variable = skim_variable) %>%
  
  kbl(., booktabs = TRUE, 
      
      caption = "Summary Statistics for categorical Variables") %>%
  
      kable_classic(full_width = TRUE)
```

Finally, I examine how staff satisfaction relates with employee churn in Table (). The output shows that employees who leave hve marginally lower average satisfaction scores. 

```{r}
df %>% 
  
  group_by(left) %>% 
  
  summarise(mean_sat = mean(satisfaction),
            
            median_sat = median(satisfaction),
            
            min_sat = min(satisfaction),
            
            max_sat = max(satisfaction)) %>% 
  
  kbl(., booktabs = TRUE, 
      
      caption = "Employee Satisfaction versus Churn") %>%
  
      kable_classic(full_width = FALSE)
```


# Which Department Has the Highest/ Lowest Turnover

In this section,We examine the employee turnover by department. First, we look at the absolute number of staff who have left each department. 

```{r}
df %>%
# we will add a column that has a unique id for each row

mutate(employee_id= row_number())  %>%

  filter(left == "yes") %>%

  group_by(department) %>%

  count(left,sort=TRUE) %>% 
  
  select(-left) %>% 
  
  kbl(., booktabs = TRUE, 
      
      caption = "Employee Churn by Department") %>%
  
      kable_classic(full_width = FALSE)
```


However, this approach may be misleading. Hence, we compute turnover. We define staff turnover as the ratio of the number of employees who left the organisation to the total number of employees in the organisation (including those who have left). using this ratio allows us to standardise employee turnover so that it is comparable across departments. 

Table () shows the number of that employee turnover is highest in the IT department, followed by logistics. Finance, followed by administration have the lowest incidence of staff turnover. However, the gap in turnover ratios is not that great. For instance, the gap in staff turnover between IT and finance is only `r glue::glue("{30.9-26.87}%")`, which indicates that staff turnover is a problem in all departments. 

```{r}

df %>% 
  
  ## Group by department
  group_by(department) %>%
  
  ## Count the people who have left/ remained
  count(left) %>%
  
  ## Get the proportion of people who left/ remained
  mutate(prop = n / sum(n) * 100 %>% round(2)) %>%
  
  ungroup() %>%
  
  filter(left == "yes") %>%
  
  arrange(desc(prop)) %>%
  
  kbl(., booktabs = TRUE, 
      
      caption = "Staff Turnover by Department") %>%
  
  kable_classic(position = "left", full_width = FALSE)
```

# Which Variables Best Predict Staff Turnover?

In this section We use statistical learning models to examine variables that best predict staff turnover.We proceed by first creating a training set and a testing set from the given data. Given that the data set has a problem of imbalance in the target (dependent) variable, `left`,We up-sample the data.We then run the following models. 

1. Logistic regression model. 
2. Decision tree model.
3. Random forest model. 
4. K-Nearest neighbours model (KNN).
5. XG-Boost Model.
5. Support vector machine (SVM) model.
6. An ensemble of all the models. 

## Creating the Training and Testing Sets. 

In this step,Wecreate a traing and testing set. The training set has 75% of the data, while the testing set has the remaning 25% of the data points. 

```{r}

## Create a split object consisting 75% of data
set.seed(200)
split_object <- initial_split(df, prop = 0.75, 
                              
                              strata = left)

## Generate the training set
df_train <- split_object %>%
  
  training()

## Generate the testing set
df_test <- split_object %>%
  
  testing()

```

## Data Pre-processsing and Feature Engineering
Next,We set up a recipe object that will allow for data pre-processing and feature engineering where required. In this step,Wemake a recipe object that does the following.

1. Up-samples the data so that the target variable (`left`) has the same proportion of workers who left and remained with the organisation. 

2. Converts all character variables into factors. 

3. In a pair of highly correlated variables, drops one of the variables with a threshold correlation of 0.85 (absolute).


```{r}

df_recipe <- recipes::recipe(left ~ ., 
                             
                             data = df_train) %>%
  
  ##We upsample the data to balance the outcome variable
  themis::step_upsample(left, 
                        
                        over_ratio = 1, 
                        
                        seed = 200) %>%
  
  ##We make all character variables factors
  step_dummy(all_nominal_predictors()) %>%
  
  ##We remove one in a pair of highly correlated variables
  ## The threshold for removal is 0.85 (absolute) 
  ## The choice of threshold is subjective. 
  step_corr(all_numeric_predictors(), 
            
            threshold = 0.85) %>%
  
  ## Train these steps on the training data
  prep(training = df_train)
```

Finally,We apply these transformations to the training and testing data sets.

```{r}
## Generate new datasets with all steps defined included
###############################
df_baked_train <- df_recipe %>%
  
  bake(new_data = NULL)

###############################
df_baked_test <- df_recipe %>%
  
  bake(new_data = df_test)
```

Next,Wefit the models and evaluate their performance. 

## Logistic Regression

I start by defining the model. 

```{r}
## Define a logistic model
logistic_model <- logistic_reg() %>%
  
  set_engine("glm") %>%
  
  set_mode("classification")
```

Next,We fit the model on the training set.

```{r, results='asis'}
logistic_results <- workflow() %>% 
  
  add_model(logistic_model) %>%
  
  add_recipe(df_recipe) %>% 
  
  fit(data = df_train)

## Show coefficients table with stars for statistical significance
stargazer::stargazer(glm(left ~ ., data = df_baked_train, 
                         
                     family = "binomial"), 
                     
                     type = "html", 
                     
                     title = "Results of Logistic Regression", 
                     
                     align = TRUE)
```

The results of the regression in Table () show the following variables being instrumental in determining whether or not staff leave the organisation.

- Promoted.
- Review.
- Tenure.
- Satisfaction.
- Salary.

The `promoted`, `tenure`, and `satisfaction` variables are major predictors staff turnover. Specifically, staff that have received a promotion have a higher likelihood of leaving, presumably for greener pastures. In contrast, staff that have higher scores in past job reviews, with a longer tenure, and a higher job satisfaction score in the past have a lower chance of leaving. Compared to staff with high salaries, staff with a lower salary are more likely to leave while those with medium salaries are more likely to stay. 

The code chunk below shows the predictions from the logistic regression on the test set.Westart using the model make predictions on the test set. Next,Webind the predictions (both probabilities and actual classes predicted) to the test set. 

```{r}
#############################################
## Get class predictions
logistic_class <- logistic_results %>% 
  
  predict(new_data = df_test, 
          
          type = "class")

#############################################
## Get probabilities
logistic_prob <- logistic_results %>% 
  
  predict(new_data = df_test, 
          
          type = "prob")

#############################################
df_test_bind <- df_test %>% 
  
  select(left) %>% 
  
  bind_cols(logistic_class, 
            
            logistic_prob)
```

Next,We generate metrics for evaluating the performance of the model.

```{r}
df_test_bind %>% 
  
  conf_mat(truth = left, 
           
           estimate = .pred_class) %>% 
  
  autoplot(type = "mosaic") + 
  
  labs(title = "Visualization of Confusion Matrix")
################################################
df_test_bind %>% 
  
  conf_mat(truth = left, 
           
           estimate = .pred_class) %>% 
  
  summary() %>% 
  
  kbl(., booktabs = TRUE, 
      
      caption = "Statistics for Logistic Regression") %>%
  
  kable_classic(position = "left", full_width = FALSE)
```

Figure below shows the area under curve

```{r}
df_test_bind %>% 
  
  roc_auc(truth = left, .pred_yes)

df_test_bind %>% 
  
  roc_curve(truth = left, .pred_yes) %>% 
  
  autoplot() + 
  
  labs(title = "ROC Curve for Logit Model (ROC AUC = 0.701)")
```


```{r}
logistic_results %>% 
  
extract_fit_engine() %>% 
  
  vip() + 
  
  labs(title = "Variable Importance: Logit Model")
```



## Decision Tree Model

We can define decision trees succinctly as follows;

>[A decision tree is a decision support tool that uses a tree-like model of decisions and their possible consequences, including chance event outcomes, resource costs, and utility. It is one way to display an algorithm that only contains conditional control statements](https://en.wikipedia.org/wiki/Decision_tree). 

The Corporate Finance Institute also offer a definition that allows us to better visualise the structure of decision trees. 

>[A decision tree is a support tool with a tree-like structure that models probable outcomes, cost of resources, utilities, and possible consequences. Decision trees provide a way to present algorithms with conditional control statements. They include branches that represent decision-making steps that can lead to a favorable result](https://corporatefinanceinstitute.com/resources/knowledge/other/decision-tree/).

Typically, a decision tree model uses recursive partitioning. Each partition arises from a decision. There are two key types of nodes. 

- The decision node: This is a node where a decision is made and a split occurs. Typically, a decision tree will split the data multiple times based on certain cut offs in the features or independent [variables](https://christophm.github.io/interpretable-ml-book/tree.html). 
- Terminal node: These are nodes that come last. They signify the peak of the tree branch and point to a decision. 

Decision trees are easy to build, read, and interpret. Also, decision trees require less data cleaning as they are minimally affected by missing values and outliers. However, decision trees can be unstable with slight changes in data and are prone to over fitting. Finally, decision trees may not be very effective when dealing with a continuous dependent variable. Hence, it is critical to tune hyper parameters for decision trees. In particular, the following parameters key:

- Tree depth:
- Cost complexity:


```{r}
decision_tree_model <- decision_tree(
  
  cost_complexity = tune(),
  
  tree_depth = tune()
  
) %>% 
  
  set_engine("rpart") %>% 
  
  set_mode("classification")
```

Next, we create a grid of values (for cost complexity and tree depth) that we shall test. The output will allow us to pick the optimal parameters. Here, R and Tidymodels pick several values that we use to tune parameters. In this case we have asked for 10 of each. 

```{r}
tree_grid <- grid_regular(
  
  cost_complexity(),
  
  tree_depth(),
  
  levels = 5
)

head(tree_grid)
```

Next, we create cross validation folds in the training data. We shall use these folds to tune the parameters. 

```{r}
set.seed(200)
cell_folds <- vfold_cv(df_train)
```

We then fit the model to the folds by defining a workflow. 

```{r}
tree_wf <- workflow() %>% 
  
  add_recipe(df_recipe) %>% 
  
  add_model(decision_tree_model)


tree_tune <- tree_wf %>% 
  
  tune_grid(
    
    resamples = cell_folds,
    
    grid = tree_grid
    
  )
```

We can now see the metrics generated in the tuning process. 

Figure () visualizes these metrics. 

```{r}
tree_tune %>%
  
  collect_metrics() %>%
  
  mutate(tree_depth = factor(tree_depth)) %>%
  
  ggplot(aes(cost_complexity, mean, color = tree_depth)) +
  
  geom_line(size = 1.5, alpha = 0.6) +
  
  geom_point(size = 2) +
  
  facet_wrap(~ .metric, scales = "free", nrow = 2) +
  
  scale_x_log10(labels = scales::label_number()) +
  
  scale_color_viridis_d(option = "plasma", 
                        
                        begin = .9, end = 0)
```

Here, we see that a shallower tree has the worst performance on both metrics- AUC and Accuracy. 

```{r}
tree_tune %>% 
  
  collect_metrics()
```


Here, we select the best tree. 

```{r}
best_tree <- tree_tune %>% 
  
  select_best("roc_auc")
  
```


```{r}
final_tree <- tree_wf %>% 
    
    finalize_workflow(best_tree) %>% 
  
    fit(data = df_train)
```


```{r}

final_tree %>% 
  
  extract_fit_engine() %>% 
  
  rpart.plot(roundint = FALSE)
```

We then do the predictions on the test set, generate metrics and and plot the ROC curve. 

```{r}
final_tree %>% 
  
  augment(new_data = df_test) %>% 
  
  conf_mat(truth = left, estimate = .pred_class) %>% 
  
  summary()
```

```{r}
final_tree %>% 
  
  augment(new_data = df_test) %>% 
  
  conf_mat(truth = left, estimate = .pred_class) %>% 
  
  autoplot() + 
  
  labs(title = "Visualisation of Confusion Matrix: Decision Tree Model")
```



```{r}
final_tree %>% 
  
  augment(new_data = df_test) %>% 
  
  roc_auc(truth = left, .pred_yes)
```



```{r}

final_tree %>% 
  
  augment(new_data = df_test) %>% 
  
  roc_curve(truth = left, .pred_yes) %>% 
  
  autoplot()
  
```


```{r}
final_tree %>% 
  
  extract_fit_engine() %>% 
  
  vip() + 
  
  labs(title = "Variable Importance: Decision Tree Model")
```


## Random Forest Model


```{r}
rand_model <- rand_forest() %>% 
  
  set_engine("ranger", importance = "impurity") %>% 
  
  set_mode("classification")
```


```{r}
## Create a workflow
random_model_fit <- workflow() %>% 
  
  add_recipe(df_recipe) %>% 
  
  add_model(rand_model) %>% 
  
  fit(data = df_train)
```

```{r}
## Predictions
random_model_fit %>% 
  
  augment(new_data = df_test) %>% 
  
  conf_mat(truth = left, estimate = .pred_class) %>% 
  
  autoplot() + 
  
  labs(title = "Visualization of Confusion Matrix: Random Forest Model")
```


```{r}
## Predictions
random_model_fit %>% 
  
  augment(new_data = df_test) %>% 
  
  conf_mat(truth = left, estimate = .pred_class) %>% 
  
  summary()
```


```{r}
random_model_fit %>% 
  
  augment(new_data = df_test) %>%
  
  roc_auc(truth = left, .pred_yes)
```

```{r}
random_model_fit %>% 
  
  augment(new_data = df_test) %>%
  
  roc_curve(truth = left, .pred_yes) %>% 
  
  autoplot()
```


```{r}
random_model_fit %>% 
  
  extract_fit_engine() %>% 
  
  vip() + 
  
  labs(title = "Variable Importance: Random Forest Model")
```

## K-Nearest Neighbours (KNN) Model

```{r}
knn_model <- nearest_neighbor(
  
  neighbors = tune()
  
) %>% 
  
  set_engine("kknn") %>% 
  
  set_mode("classification")
```


```{r}
knn_wf <- workflow() %>% 
  
  add_recipe(df_recipe) %>% 
  
  add_model(knn_model)
```

```{r, cache = TRUE}
neighbours_grid <- tibble(neighbors = seq(from = 5, to = 100, 
                                          
                                          by = 5))


knn_tune <- knn_wf %>% 
  
  tune_grid(
    
    resamples = cell_folds,
    
    grid = neighbours_grid
    
  )
```


```{r}
knn_tune_best <- knn_tune %>% 
  
  select_best("roc_auc")
```


```{r}
final_knn_model <- knn_wf %>% 
  
  finalize_workflow(knn_tune_best) %>% 
  
  fit(data = df_train)
```

```{r}
final_knn_model %>% 
  
  augment(new_data = df_test) %>% 
  
  conf_mat(truth = left, estimate = .pred_class) %>% 
  
  autoplot()
```

```{r}
final_knn_model %>% 
  
  augment(new_data = df_test) %>% 
  
  conf_mat(truth = left, estimate = .pred_class) %>% 
  
  summary()

```


```{r}
final_knn_model %>% 
  
  augment(new_data = df_test) %>% 
  
  roc_auc(truth = left, .pred_yes)

```


```{r}
final_knn_model %>% 
  
  augment(new_data = df_test) %>% 
  
  roc_curve(truth = left, .pred_yes) %>% 
  
  autoplot()
```

## Support Vector Machines (SVMs)

The idea behind SVMs is to define a hyperplane that distinctly divides data points into their respective categories. Where there are two categories, then this hyperplane will be a line. If we have three classes, then we have a plane. Beyond this we have hyperplanes in N-Dimensions. In many cases, there exists more than one hyperplane to distinguish between the categories. In that case, we select the hyperplane that gives the [largest separation or margin between the categories](https://www.geeksforgeeks.org/support-vector-machine-algorithm/). In cases where classes are not linearly separable, we can create a new variable that is a function of the distance from the origin called the `kernel`. 

We implement the SVM model. 

```{r}
svm_model <- svm_rbf(
  
  cost = tune(),
  
  rbf_sigma = tune()
  
) %>% 
  
  set_mode("classification") %>% 
  
  set_engine("kernlab")
```


```{r}
svm_wf <- workflow() %>% 
  
  add_recipe(df_recipe) %>% 
  
  add_model(svm_model)

```


```{r, cache = TRUE}
ctrl <- control_grid(verbose = FALSE, save_pred = TRUE)

roc_vals <- metric_set(roc_auc)

svm_tune <- svm_wf %>% 
  
  tune_grid(
    
    resamples = cell_folds,
    
    control = ctrl,
    
    metrics = roc_vals
    
  )
```



```{r}

svm_tune_best <- svm_tune %>% 
  
  select_best()

#############################
svm_final_model <- svm_wf %>% 
  
  finalize_workflow(svm_tune_best) %>% 
  
  fit(data = df_train)
```


```{r}
svm_final_model %>% 
  
  augment(new_data = df_test) %>% 
  
  conf_mat(truth = left, estimate = .pred_class) %>% 
  
  autoplot()
```

```{r}
svm_final_model %>% 
  
  augment(new_data = df_test) %>% 
  
  conf_mat(truth = left, estimate = .pred_class) %>% 
  
  summary()
```



```{r}
svm_final_model %>% 
  
  augment(new_data = df_test) %>% 
  
  roc_auc(truth = left, .pred_yes)
```



```{r}
svm_final_model %>% 
  
  augment(new_data = df_test) %>% 
  
  roc_curve(truth = left, .pred_yes) %>% 
  
  autoplot() + 
  
  labs(title = "ROC Curve for SVM Model (ROC AUC = 0.901)")
```

## Extreme Gradient Boosting (XGBoost) Model

In this section, we borrow substantially from [Julie Silge](https://juliasilge.com/blog/xgboost-tune-volleyball/) material on tuning XGBoost models. The material in in both audio and written form.


## Ensemble


# Recomendations 


# Conclusion

